{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We'll start using tensorflow and keras(uses tensorflow, MxNet and other deep learning library as a backend).\n",
    "\n",
    "Tensorflow is a big libray. More you use it and encounter more use cases better you will learn how to translate your idea into code(computational graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please look at the posted presentation. \n",
    "## It contains summary of google tensorflow whitepaper\n",
    "http://download.tensorflow.org/paper/whitepaper2015.pdf, I read sometime back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short it is a library to generate/build a computation graph.\n",
    " - One writes codes to specify abstract computation like addition and matrix multiplication\n",
    " - One can feed the actual data later to evaluate different nodes in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But Why to build graph for doing computations?\n",
    "Building graph before doing actual computation provides lot of optimization benefits like\n",
    "- Common Subexpression Elimination. Avoids redundant computation\n",
    "- optimize operations\n",
    "\n",
    "Graph optimization is itself a big areas of research and we can benefits from this research without worrying about actual research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and what is a tensor and where it fits in above computation graph lingo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want\n",
    "\n",
    "  - to go deep down what a tensor is, we need to take a deeper dive.\n",
    "      + [Intoduction to Tensor](https://math.stackexchange.com/questions/10282/an%C2%ADintroduction%C2%ADto%C2%ADtensors?%20noredirect=1&lq=1)\n",
    "      + For a lighter reading just skim thought https://en.wikipedia.org/wiki/Tensor\n",
    "     \n",
    "\n",
    "They represent linear relationships between vector and other tensors.\n",
    "\n",
    "Think about how matrix(a kind of tensor) $M_{n,m}$ maps a **m-dimentional** vector $v_{m,1}$ into another\n",
    "vector **n-dimentional** $(Mv)_{n,1}$.\n",
    "\n",
    "<font color = \"red\">Note: Every multi-dimentional thing is not a tensor. Tensor represent function. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may not have thought about scalar and vector and matrix in this way but\n",
    "- scalars $x \\in \\mathbb{R}$  0th-order tensor\n",
    "- Vector are 1th order tensor\n",
    "- Matrix are 2 nd order tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So tensorflow builds a computation graph where **node** in the graph represent computation and **input and output** represent the flow of tensors. It is one way to think about tensorflow or **flow of tensors(scalar, vector, matrix etc.) via  computation nodes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at a simple graph from the google paper\n",
    " <img src= \"computation_graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In previous graph think of  weight matrix(parameters to learn) $W$, feature vector $X$, scalar $b$ as tensors.\n",
    "- Various nodes MatMul, Add etc as  computations you want to perform\n",
    "- Input and output flow along the edge of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the above things are great and but one of the biggest advantage of  tensorflow like graph building libraries is that they do automatic differentations too\n",
    "<img src= \"gradient_computation.png\">\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One need not to worry about building gradient computation nodes. As one keep adding various part of the graph(left graph in above picture), library automatically keep building differentiation graph(right graph in the picture) too.\n",
    "\n",
    "Remember some time there is no close form solution to find parameters $W$ which maximizes likelihood or log likelihood function $C(W)$ (**MLE estimation procedure**) as in logistic regression. We showed that if function $C(W)$  is differentiable\n",
    "one can use an iterative procedure called **gradient descent** to update the parameters.\n",
    "\n",
    "$W_{k+1} = W_k + \\eta \\frac{dC(W)}{W}$\n",
    "\n",
    "If function $C(W)$ is convex, and one uses right step size $\\eta \\in \\mathbb{R}^{+}$(set of all positive numbers), one is guaranteed to find optimal value of parameter $W$. In logistic regression case $C(W)$ is cross entropy and measure how well the logistic machine is performing currently by computing the loss $C(W)$. We want to find parameter $W$ of logistic model which minimize the loss $C(W).$\n",
    "\n",
    "Note: \n",
    "- For initial step $k=0$, in general one can use any random value for parameters $W_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I hope by now you are somewhat convinced that using computation graph building libraries are quite useful and powerful in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started with using tensorflow(creating various type of nodes and tensors).\n",
    "\n",
    "https://www.tensorflow.org/guide/\n",
    "\n",
    "**Please follow instruction in course webpage to install tensorflow and keras.**\n",
    "\n",
    "https://github.com/psnegi/ml_s2018#tensorflow-and-keras\n",
    "\n",
    "[Keras](https://keras.io/) is a well-thought-out  high level computational graph building library. One need not to write a lot of boiler plate code.\n",
    "\n",
    "<font color ='red'> Following code will not work if you haven't installed tensorflow </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # importing tensorflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating a constant of type string\n",
    "It takes no input and outputs stored tensor. Also they are immutable(can't change the value once defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_ml = tf.constant('Hello machine learning, probabilistic perspective')\n",
    "print(hello_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get value out we need to run it using a session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    value = sess.run(hello_ml)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 (.5 point) Create a constant tensor with value \"tensorflow\" and print the value by using sess as done above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variable\n",
    "\n",
    "Training a machine learning model is nothing but learning the paramters of a choosen model.\n",
    "\n",
    "We need a way to define variable representing the parameters. They are mutable. When we train the model, value stored in variable changes reflecting the learned parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do a simple matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = tf.Variable([[1, 2, 1],[2, 2, 2]])\n",
    "I = tf.Variable([[1, 0 ,0],[0, 1, 0], [0, 0, 1]])\n",
    "random_normal = tf.Variable(initial_value= tf.random_normal([10], mean = 1.0, stddev= 0.1))\n",
    "# Please keep using . tab or shit tab to find method and argument list respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = tf.matmul(M, I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uncomment this cell and try to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#with tf.Session() as sess:\n",
    "#    prod_value, random_normal_samples = sess.run([prod, random_normal])\n",
    "#    print(prod_value)\n",
    "#    print(random_normal_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to run the pervious cell you will get **FailedPreconditionError**. We always need to initilaize our variable before using them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # always need to initialize\n",
    "    prod_value, random_normal_value = sess.run([prod, random_normal])\n",
    "    print(prod_value)\n",
    "    print(random_normal_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code section see different ways to give initial values to variables.\n",
    "Checkout various methods here\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to create a variable is to call the **tf.get_variable function.**\n",
    "\n",
    "We'll not use tf.Variable directly hence onward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder\n",
    "As the name specifies we can create place holder in the graph withour  creating actual tensor.\n",
    "But when you run the graph you need to feed the right shape and type of the tensor\n",
    "\n",
    "**See below how we created x as a placeholder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"M\", reuse=tf.AUTO_REUSE):\n",
    "    M = tf.get_variable(name = 'matrix', initializer=  tf.constant([[1.0, 2, 1],[2, 2, 2]]))\n",
    "x = tf.placeholder(tf.float32, shape=(3, 1))# just tell the type and shape\n",
    "matrix_vector_mul = tf.matmul(M,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # feeding thave value of place holder x at run time\n",
    "    print(sess.run(matrix_vector_mul, feed_dict={x: np.array([[1],[0.0], [0.0]])}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do ridge linear regression(predicting continous value) using tensorflow on \n",
    "\n",
    "Boston house price dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's download dataset $\\mathcal{D} = \\{(\\mathbf{x_i}, y_i) \\}_{i=1}^{N}$ containing features $\\mathbf{x_i}$  and target value $y_i$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for doing eploratory data analysis\n",
    "import seaborn as sns # statistical visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "# to make graphics inline\n",
    "%matplotlib inline \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using pandas read_csv and giving name for the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_data = load_boston()\n",
    "print(boston_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)\n",
    "df['target'] = boston_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 (.5 point) create a panda Series of size df.shape[0]  with value 1 and name CONST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_df = ###? write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([const_df, df], axis=1) # adding 1 to all the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Summary and data sanity check\n",
    "\n",
    "Please read pandas **isnull** and **any** functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to make sure values in different columns are not missing\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As per the above output none of the columns have  any null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making sure datatype is also good, so that relevant algebra on columns make sense\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "validation_size = 0.40\n",
    "seed = 3\n",
    "train_df, valid_test_df = train_test_split(df, test_size=validation_size, random_state=seed)\n",
    "valid_df, test_df = train_test_split(valid_test_df, test_size=.5, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = '#FF5733'>Can you guess why we splitted data into train, validation and test set? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ridge regression remember we have to tune parameter $\\lambda$. It controls strength of regularization(How small each $w_i$ should be). There is not formula for it given the data. We need to tune it.\n",
    "\n",
    "We will use validation data to search a grid of values to find optimal $\\lambda$ using validation data as we can't touch test data to evaluate performance of the linear model.\n",
    "\n",
    "<font color = '#FF5733'>Test data works as proxy for unseeen data. Only touch it when you have selected a final model. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape, valid_test_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning we would like uncorrelated features. Let see how our attribute/features are correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last column tell the predictivity of the various attribute\n",
    "\n",
    "Correlation is not the only way to measure predictivity of the features. One can use information theoretic ideas\n",
    "like mutual inofmation etc. to find more predictive(powerful) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,14)) \n",
    "sns.heatmap(train_df.corr(), annot=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually we can see lot of correlation among attribute(like DIS and INDUS) and some attribute being more predictive than other(looks like LSTAT is more predictive)\n",
    "\n",
    "We can select feature based on correlation and predictivity. It is very important activity to make sure features are uncorrelated. Once can use PCA, ICA, dimentionality reduction, manifold learning to create uncorrelated features.\n",
    "\n",
    "We can go head and pick the feature based on correlation and predictivity but\n",
    "but let's go head and do learn ridge regression to take care of correlation among features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build design matrix X containing observations along the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are doing random selection\n",
    "selected_feature =['CONST', 'CRIM', 'INDUS', 'NOX', 'DIS', 'RAD', 'TAX','PTRATIO', 'B', 'LSTAT']\n",
    "X_train = train_df[selected_feature].values\n",
    "y_train = train_df['target'].values\n",
    "X_valid = valid_df[selected_feature].values\n",
    "y_valid = valid_df['target'].values.reshape((-1,1))\n",
    "X_test =  test_df[selected_feature].values\n",
    "y_test = test_df['target'].values.reshape((-1,1))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0:3,:], y_train[0:3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution of ridge regression is given by\n",
    "$$\\hat{w}_{ridge} = (\\lambda I_D + X^TX)^{-1}X^T y$$\n",
    "\n",
    "For us X is X_train and y is y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build a computational graph to find $\\hat{w}_{ridge}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 ( .5 point) create a place holder of y. This represents the  vector containing all the y_i values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape= X_train.shape, name='input_training_features')\n",
    "y = ###??? write your code here\n",
    "l = tf.placeholder(tf.float32, shape= [], name='regularization_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build ridge formula(computation)\n",
    "first\n",
    "\n",
    "$\\lambda I_D + X^TX$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.multiply(l, tf.eye(X_train.shape[1])) + tf.matmul(tf.transpose(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 4 (1.5 point) Write code to build\n",
    "$(\\lambda I_D + X^TX)^{-1} X^T$\n",
    "\n",
    "Note that temp already has $\\lambda I_D + X^TX$\n",
    "\n",
    "Hint: search for how to represent matrix inverse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ridge = ###write code here\n",
    "print(temp_ridge.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matmul needs both the arguments to have 2 dimension. Hence adding second dimension. I don't why? In numpy this is not an issue. Let me know if have better solution to mulitply matrix and vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_weights = tf.matmul(temp_ridge, tf.expand_dims(y,1))\n",
    "print(ridge_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run our computation graph, feeding actual data\n",
    "<font color = 'green'>see the feed_dict argument. How we are providing the actual data so that required graph dependency is statisfied </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-d  grid of $\\lambda$ values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [1e-20, 1e-10, 1e-5, 1e-4, 1e-3,1e-1, 1, 5.0, 10, 50, 100]\n",
    "print(lambdas)\n",
    "print(type(lambdas[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a pandas dataframe to store $\\lambda$ and learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind =['lambda_{}'.format(la) for la in lambdas]\n",
    "column_names = ['lambda', 'mse']+ ['w_{}'.format(i) for i in range(X_train.shape[1])]\n",
    "print(ind)\n",
    "print(column_names)\n",
    "coeff_matrix = pd.DataFrame(index=ind, columns=column_names, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_matrix.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_matrix.head()\n",
    "# we haven't filled values in different columns.  NaN is ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification we used accuracy as a measure of how well our model performed.\n",
    "\n",
    "But how to measure regression model performance for various value of $\\lambda$?\n",
    "\n",
    "For linear model we can calculate the MSE (mean square error)\n",
    "\n",
    "Look at this link to learn about various other metrics to use for model selection and evaluation\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 5 (1 point)  In the feed_dict part feed the actual value for y and l(regularization_weight) tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    for i, reg in enumerate(lambdas):\n",
    "        # running the graph and feeding actual data\n",
    "        ridge_weight_value = sess.run(ridge_weights, feed_dict={X:X_train, ##write code here##, ###write code here#})\n",
    "        # Let's evaluate the performance using MSE on y_valid, y_valid_prediction data\n",
    "        #print(ridge_weight_value)\n",
    "        y_valid_pred= np.dot(X_valid, ridge_weight_value)\n",
    "        # See how we can evaluate l_2 norm in numpy\n",
    "        mse = (np.linalg.norm(y_valid - y_valid_pred ,ord=2)/(len(y_valid)))**2\n",
    "        coeff_matrix.iloc[i, 0] = reg\n",
    "        coeff_matrix.iloc[i, 1] = mse\n",
    "        print(mse)\n",
    "        coeff_matrix.iloc[i, 2:] = ridge_weight_value.T\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.shape, y_valid_pred.shape, ridge_weight_value.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coeff_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note how weights are decreasing as lambda increases in the bottom part for $w_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on minimum mse pick corresponding weight vector to evalue mse on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 6 (1 point) Select the index name you think has minumum value of mse in corresponding row\n",
    "\n",
    "hint: Like if lambda_0.0001 is minimum mse then\n",
    "\n",
    "selected_index = 'lambda_0.0001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_index = ### Write string index here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See what would have happened if you choose the average of y_valid for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = np.mean(y_valid, axis=0)\n",
    "print(mean_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This would have been our MSE in this base scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(y_valid -mean_val, ord=2)**2/len(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let see how well we did on truly unseen data(never touched during building model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_weights =  coeff_matrix.loc[selected_index:].values.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred= np.dot(X_test, selected_weights)\n",
    "print(y_test_pred.shape, y_test.shape)\n",
    "print(type(y_test_pred), type(y_test))\n",
    "test_mse = np.linalg.norm(y_test - y_test_pred ,ord=2)**2/(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See what we get if we used sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 (1 point) Use Ridge from sklearn, fit on training data and comute the MSE error on test set. Keep normalize=False when instantiating Ride class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# Write code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Also one can see how good is  the linear model by analysing the error on training data. Error $\\epsilon_i$ should be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred= np.dot(X_train, coeff_matrix.loc[selected_index, 'w_0':].values.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y_train_pred - y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(error, line='s')\n",
    "# looks line not a great fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
