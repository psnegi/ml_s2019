{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's flow some tensors(vector, matrices etc.) for multi class logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf # need to import the right package\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Here we will build a simple logistic regression model to classify mnist data set\n",
    "\n",
    "Task: **classify the hadwritten digit label $\\in \\{1, 2, \\cdots, 9 \\}$ using pixel from the digit image.**\n",
    "\n",
    "This is classification(discrete label output) problem and multi class logistic regression can be used here\n",
    "\n",
    "Check   [Yann LeCun](http://yann.lecun.com/),      http://yann.lecun.com/exdb/mnist/  website for details about mnist dataset etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify everthing looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} training example with shape {}'.format(mnist.train.num_examples, mnist.train.images[0].shape))\n",
    "print('{} test example with shape {}'.format(mnist.test.num_examples, mnist.test.images[0].shape))\n",
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains 55000 training and 10000 test example for handwritten digits in vectorized form(take pixel along rows and stack them in a column vector).\n",
    "\n",
    "Images are 28X28 gray scale. After vectrorization we have 784 dimentional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28*28 == 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES = 10\n",
    "X_DIM = 28\n",
    "Y_DIM = 28\n",
    "import numpy as np\n",
    "unique_label = np.unique(np.argmax(mnist.train.labels, 1))\n",
    "print(unique_label)\n",
    "assert NUM_CLASSES == len(unique_label), 'number of label does not match'\n",
    "assert X_DIM*Y_DIM == mnist.train.images[0].size, 'total pixel does not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's randomly view some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command so that images are inline in notebook\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt # visualization package in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # to make sure we have deterministic results on each iteration\n",
    "NUM_FIG_DISP = 16\n",
    "nrow = NUM_FIG_DISP//2\n",
    "ncol = NUM_FIG_DISP//2\n",
    "rand_ind = np.random.randint(mnist.train.num_examples, size=NUM_FIG_DISP)\n",
    "print(rand_ind)\n",
    "plt.figure(1,figsize= (16, 16))\n",
    "plt.gray()\n",
    "for idx, image_index in enumerate(rand_ind):\n",
    "    plt.subplot(nrow,ncol,idx +1)\n",
    "    # Have to reshape to 28x28 for display\n",
    "    plt.imshow(np.reshape(mnist.train.images[image_index], (X_DIM, Y_DIM)) )\n",
    "    plt.title('label is {}'.format(np.argmax(mnist.train.labels[image_index])))  \n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define  multi class logistic regression  model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember for each class $c \\in \\{1, 2, \\cdots, K= 10   \\}$ we need to compute\n",
    "\n",
    " probablity\n",
    " <font size = 6>\n",
    " $P(y = c|x) = \\frac{\\exp(w_{oc} + w_c^Tx)}{\\sum_i^K \\exp(w_{oi} + w_i^Tx) }$\n",
    " </font>\n",
    " \n",
    " \n",
    " Note the one can either add 1 to features or work with $D+1$ dimentional features or add class bias $w_{oc}$ term for each class explicility as done in above formula.\n",
    " \n",
    " We can keep each weight vector $[w_{c}]_{D \\times 1}$ in $D\\times K$ matrix $W$, \n",
    " $$W = {\\begin{bmatrix} w_1, w_2, \\cdots, w_K \\end{bmatrix}}_{D \\times K}$$\n",
    " \n",
    " and class biases  in vector  $W_o = {\\begin{bmatrix} w_{o1}\\\\ w_{02} \\\\ \\vdots \\\\ w_{0K} \\end{bmatrix}}_{K \\times 1}$. $D$ is data dimension.\n",
    " \n",
    " \n",
    " Using matrix operation we can calculate each class probability for given example $x_{784 \\times 1}$ by doing\n",
    " \n",
    " $softmax(W_o +  W^T x)$ where \n",
    " \n",
    " $$W_o + W^Tx = \\begin{bmatrix} w_{o1}\\\\ \\vdots \\\\ w_{oK} \\end{bmatrix} + \\begin{bmatrix}  w_1^T\\\\ \\vdots \\\\  w_K^T \\end{bmatrix} x = \\begin{bmatrix}w_{01} + w_1^Tx\\\\ \\vdots \\\\ w_{0K} + w_K^Tx \\end{bmatrix}$$\n",
    " \n",
    " Also remember that \n",
    " <font size = 5>\n",
    " \n",
    " $$softmax(\\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_K  \\end{bmatrix}) = \\begin{bmatrix} \\frac{\\exp(z_1)}{\\sum_i^K \\exp(z_i)} \\\\ \\frac{\\exp(z_2)}{\\sum_i^K \\exp(z_i)} \\\\ \\vdots \\\\ \\frac{\\exp(z_K)}{\\sum_i^K \\exp(z_i)}  \\end{bmatrix} $$ </font>\n",
    " \n",
    " Convince yourself that after applying softmax we will get the formula at the beginning of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to  code above equations using tensorflow computational graph\n",
    "\n",
    "When we define placeholder, we don't need to specify number of examples dimension.\n",
    "See below we use **None**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 (.5 point) Write the placeholer of True labels Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PIXELS_PER_SAMPLE = X_DIM*Y_DIM\n",
    "\n",
    "X =  tf.placeholder(tf.float32, [None, PIXELS_PER_SAMPLE])\n",
    "Y = ## Write code here\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 (.5 point) Create a variable using get_variable for Vector $W_o$ as defined above and use zeros initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"multi_class_logistic_model\", reuse=tf.AUTO_REUSE):\n",
    "    W = tf.get_variable('Weight_matrix', initializer = tf.random_normal(shape = (X_DIM*Y_DIM, NUM_CLASSES)))    \n",
    "    W_o= ### Write your code here\n",
    "    print(X.shape, W.shape, W_o.shape)\n",
    "    # we have to do X traspose as examples are along the row and we need them along columns\n",
    "\n",
    "    Y_pred = tf.matmul(tf.transpose(W), tf.transpose(X))  + W_o\n",
    "    \n",
    "print('shape of prediction vector is {}'.format(Y_pred.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ? represent free dimension so let transpose it again to keep free dimention first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_pred = tf.transpose(Y_pred)\n",
    "print('shape of prediction vector is {}'.format(Y_pred.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<font color = \"red\">Also above code can we written(most of the time you will see tensorflow code other way around) without so much transpose operations but then we have to think of above equations in transpose sense </font> \n",
    "\n",
    "It doesn't matter how you keep examples/weight in matrix(row or columns fashion). Just be careful about interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's convert this score vector of 10 into probability vector using softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 (.5 point)  Use softmax function from tensorflow to convert Y_pred to probability vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_prob = ### Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build a loss/cost/objective function to measure how good we are doing\n",
    "\n",
    "We use cross entopy as discussed in the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 (1 point) Using log, reduce_mean function from tensorflow, combine predicted probability tensor Y_pred_prob , and true probabilty place holder tensor Y to calculate cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ## Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build an accuracy measure\n",
    "\n",
    "We can also build the accuracy calculation in the graph. We will run this when we need to calculate accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 (1 point) build accuracy node in the graph\n",
    "\n",
    "Hint: use equal and argmax(picking the index of high probability) , cast and reduce_mean function from tensorflow. You may have to write 3 line of code. keep final tensor name as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = ## Write you code here # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let create an optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember some time there is no close form solution to find parameters $W$ which maximizes likelihood or log likelihood function $C(W)$ (**MLE estimation procedure**) as in logistic regression. We showed that if function $C(W)$  is differentiable\n",
    "one can use an iterative procedure called **gradient descent(GD)** to update the parameters.\n",
    "\n",
    "$W_{k+1} = W_k + \\eta \\frac{dC(W)}{W}$\n",
    "\n",
    "where $C(W) = \\sum_{i=1}^{N}$cross_entropy(true_probability_i, machine_predicted_probability_i)\n",
    "\n",
    "- To update parameter we have to compute $C(W)$ for all the example at every step. This can be\n",
    "computationalty expensive if we have millions of example\n",
    "\n",
    "One extreme is shuffle all the example and use one example(cost/loss of one example) at a time. This is called\n",
    "\n",
    "**Stochastic GD**\n",
    "\n",
    "for i in range(N):\n",
    "    $W_{k+1} = W_k + \\eta \\frac{dC(W)_i}{W}$\n",
    "    \n",
    "As you can guess with this approach our search path for parameter search can be quite noisy. There are lot of other mathematical questions we need to ask,\n",
    "- like will learned parameters  be correct?\n",
    "- Will cost decrease or not?\n",
    "\n",
    "In short there are gurantees if cost function is convex.\n",
    "\n",
    "We can take a midway approach called **Mini-batch gradient**, where we use small portion of total example at each step to update the parameters $W$\n",
    "\n",
    "Lingo we need to get used to is \n",
    "\n",
    "- size of Mini-batch is called batch size\n",
    "- As you can guess we are not using all the example in each iteration/step, hence we need to make multiple\n",
    "  iteration of batch size so that optimization alogorithm has seen all the example.\n",
    "- An Epoch is a complete pass through all the training data. An Epcoh will contain multiple iteration of batch size examples\n",
    "- We use multiple pass through training examples(epoch) to learn value of parameter $W$. Also we need to shuffle examples at the start of  new epoch.\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 6 (1 point) use GradientDescentOptimizer from tensorflow to create optimizer op in the graph. You need to specify learning rate and loss to minimize. You already build loss (using cross entropy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ## Write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When we run above *opt*(optimizer node) node it calculates loss, gradient and updates the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.train.images[0].dtype)\n",
    "print(mnist.train.labels[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets run the model and see how it performing\n",
    "\n",
    "Note here I am using test data as validation data here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 (1 point) run opt, loss, accuracy node using sess in the following code.\n",
    "\n",
    " you also need to feed actual data for X and Y placeholder\n",
    " \n",
    " write code in \"Write your code here\" line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for eidx in range(NUM_EPOCHS):\n",
    "        epoch_acc, epoch_loss = [], []\n",
    "        for bidx in range(mnist.train.num_examples// BATCH_SIZE):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            xs = xs.astype(np.float32)\n",
    "            _, train_loss, train_acc= ### Write your code here\n",
    "            if (bidx+1)%100 == 0: # print result every 100 batch\n",
    "                print('epoch {} training batch {} loss {} accu {}'.format(eidx +1 , bidx +1, train_loss, train_acc))\n",
    "            epoch_acc.append(train_acc)\n",
    "            epoch_loss.append(train_loss)\n",
    "        print('##################################')\n",
    "        val_acc, val_loss = sess.run([accuracy, loss],\n",
    "            feed_dict= {X:mnist.test.images.astype(np.float32), Y: mnist.test.labels})\n",
    "        print('epoch {} # test accuracy {} $ test loss {}'.format(eidx +1, val_acc, val_loss ))\n",
    "        print('##################################') \n",
    "        # Let keep epoch level values for plotting\n",
    "        train_losses.append(np.mean(epoch_loss))\n",
    "        train_accuracies.append(np.mean(epoch_acc))\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, NUM_EPOCHS+1),  train_losses)\n",
    "plt.plot(range(1, NUM_EPOCHS+1),  val_losses)\n",
    "plt.legend(['train loss','validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 8 (.5 point) plot train and val accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In deep leaning section we will beat current accuracy numbers"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
